{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# %%\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "This script runs the multi GAN and allows you to step through each part\n",
    "# divide y by exposure in xpxixpy\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "# import modules\n",
    "import torch.optim as optim\n",
    "from patsy import dmatrices\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "## Import created modules\n",
    "from Functions.MC_WGAN_GP.gan_scripts.auto_loader import PolicyDataset\n",
    "from Functions.MC_WGAN_GP.gan_scripts.generator2_v2 import Generator2\n",
    "from Functions.MC_WGAN_GP.gan_scripts.discriminator2_v3 import Discriminator2\n",
    "from Functions.MC_WGAN_GP.gan_scripts.gradiant_penalty import calculate_gradient_penalty\n",
    "from Functions.MC_WGAN_GP.gan_scripts.undo_dummy import back_from_dummies\n",
    "\n",
    "from torch.autograd.variable import Variable\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-05-07T21:41:13.604841900Z",
     "start_time": "2023-05-07T21:41:02.817626700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "devid = torch.cuda.current_device()\n",
    "print(f\"Devid: {devid}\")\n",
    "torch.cuda.set_device(devid)\n",
    "print(torch.cuda.get_device_name(devid))\n",
    "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(dev)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train = pd.read_pickle(\"./data/gan_dataprep/train_gan.pickle\")\n",
    "val = pd.read_pickle(\"./data/gan_dataprep/val_gan.pickle\")\n",
    "pol_dat = train\n",
    "formula = 'ClaimNb ~ VehBrand + VehGas + Region + AreaGLM + VehPower + VehAge + DrivAge + DensityGLM + BonusMalus'\n",
    "# %%\n",
    "# Wrangle train data\n",
    "td = back_from_dummies(train)\n",
    "td['ClaimNb'] = td['ClaimNb'].astype('float').astype('int')\n",
    "y_real, X_real = dmatrices(formula,\n",
    "                           data=td,\n",
    "                           return_type='dataframe')\n",
    "\n",
    "\n",
    "def xpxixpy(X, y):\n",
    "    return np.dot(np.linalg.inv(np.dot(X.T, X)), np.dot(X.T, y))\n",
    "\n",
    "\n",
    "xy = xpxixpy(X_real, y_real)\n",
    "disc_add_rows = xy.shape[0]\n",
    "\n",
    "# Fit a poisson Model\n",
    "poisson_mod = sm.GLM(y_real, X_real, family=sm.families.Poisson(), offset=td['Exposure']).fit()\n",
    "original_params = poisson_mod.params\n",
    "\n",
    "lower = poisson_mod.params - 1.96 * poisson_mod.bse\n",
    "upper = poisson_mod.params + 1.96 * poisson_mod.bse\n",
    "\n",
    "# Wrangle Test Data\n",
    "test2 = back_from_dummies(val)\n",
    "test2['ClaimNb'] = test2['ClaimNb'].astype('float').astype('int')\n",
    "y_test, X_test = dmatrices(formula,\n",
    "                           data=test2,\n",
    "                           return_type='dataframe')\n",
    "y_test_resp = np.squeeze(y_test) / np.squeeze(test2['Exposure'])\n",
    "\n",
    "# make predictions on test data with models trained on train data\n",
    "real_pois_preds = poisson_mod.predict(X_test)\n",
    "# %%"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This next section contains everything that we can tune in the GAN\n",
    "\"\"\"\n",
    "\n",
    "# Information about the size of the data\n",
    "data_size = pol_dat.shape[1]  # number of cols in pol_dat\n",
    "var_locs = [0, 1, 2, 3, 4, 5]  # tells us where the continous variables are\n",
    "\n",
    "# parameters\n",
    "z_size = 100  # how big is the random vector fed into the generator\n",
    "# we should only need the 55?\n",
    "batch_size = 200000\n",
    "temperature = None  # comes into play with the categorical activation see multioutput.py\n",
    "\n",
    "# Generator tuning\n",
    "gen_hidden_sizes = [100, 100, 100]\n",
    "gen_bn_decay = .90\n",
    "gen_l2_regularization = 0\n",
    "gen_learning_rate = 0.01\n",
    "noise_size = z_size\n",
    "output_size = [1, 1, 1, 1, 1, 1, 5, 11, 2, 22, 6]  # how many categories with in each variable\n",
    "\n",
    "assert sum(output_size) == data_size\n",
    "\n",
    "# Discriminator tuning\n",
    "disc_hidden_sizes = [data_size, data_size]\n",
    "disc_bn_decay = .90\n",
    "critic_bool = True  # if false then between 0 and 1\n",
    "mini_batch_bool = False\n",
    "disc_leaky_param = 0.2\n",
    "disc_l2_regularization = 0\n",
    "disc_learning_rate = 0.01\n",
    "penalty = 10  ## deals with gradiant penalty"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "auto_data = PolicyDataset(pol_dat, var_locs)\n",
    "auto_loader = DataLoader(auto_data,\n",
    "                         batch_size=batch_size,\n",
    "                         pin_memory=True,\n",
    "                         shuffle=True,\n",
    "                         num_workers=7\n",
    "                         )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PolicyDataset(Dataset):\n",
    "    def __init__(self, data, cont_locs, small_test = None):\n",
    "        self.policy = data.drop(data.columns[cont_locs], axis=1, inplace = False)  \n",
    "        self.cont = data.iloc[:,cont_locs]\n",
    "        self.small_test = small_test\n",
    "        self.cont_locs = cont_locs\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        if len(self.cont_locs) > 0:\n",
    "            return [torch.from_numpy(self.policy.iloc[index].values).float(),\n",
    "                  torch.from_numpy(self.cont.iloc[index].values).float()]\n",
    "        else:\n",
    "            return [torch.from_numpy(self.policy.iloc[index].values).float(),\n",
    "                  0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.iloc[:,cont_locs]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for c1, c2 in auto_loader: \n",
    "    print('ok')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "auto_loader.data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#loop = tqdm(total=epochs, position=0, leave=False)\n",
    "for epoch in range(epochs):\n",
    "    for d_epoch in range(disc_epochs):\n",
    "        print(f'Epoch: {epoch}, D-epoch {d_epoch}')\n",
    "        for c1, c2 in auto_loader:  # c1 is continous variables and c2 is the categorical variables\n",
    "            print('c1c2')\n",
    "            batch = torch.cat([c2, c1], 1)\n",
    "            batch = batch.to(dev)\n",
    "            optim_disc.zero_grad()\n",
    "\n",
    "            # train discriminator with real data\n",
    "            real_features = Variable(batch).to(dev)\n",
    "            real_pred = discriminator(real_features)\n",
    "            # the disc outputs high numbers if it thinks the data is real, we take the negative of this\n",
    "            # Because we are minimizing loss\n",
    "            real_loss = -real_pred.mean(0).view(1)\n",
    "            real_loss.backward()\n",
    "\n",
    "            # then train the discriminator only with fake data\n",
    "            noise = Variable(torch.FloatTensor(len(batch), z_size).normal_()).to(dev)\n",
    "            fake_features = generator(noise, training=True)\n",
    "            fake_features = fake_features.detach().to(dev)  # do not propagate to the generator\n",
    "            fake_pred = discriminator(fake_features)\n",
    "            fake_loss = fake_pred.mean(0).view(1)\n",
    "            fake_loss.backward()\n",
    "\n",
    "            # this is the magic from WGAN-GP\n",
    "            gradient_penalty = calculate_gradient_penalty(discriminator.to(dev), penalty, real_features.to(dev), fake_features.to(dev))\n",
    "            gradient_penalty.backward()\n",
    "\n",
    "            # finally update the discriminator weights\n",
    "            optim_disc.step()\n",
    "\n",
    "            disc_loss = real_loss + fake_loss + gradient_penalty\n",
    "            disc_losses = disc_loss.item()\n",
    "            # Delete to prevent memory leakage\n",
    "            del gradient_penalty\n",
    "            del fake_loss\n",
    "            del real_loss\n",
    "            del disc_loss\n",
    "            del real_features\n",
    "            del real_pred\n",
    "            del noise\n",
    "            del fake_features\n",
    "            del fake_pred\n",
    "\n",
    "    for g_epoch in range(gen_epochs):\n",
    "        print(f'Epoch: {epoch}, D-epoch {g_epoch}')\n",
    "        optim_gen.zero_grad()\n",
    "\n",
    "        noise = Variable(torch.FloatTensor(len(batch), z_size).normal_()).to(dev)\n",
    "        gen_features = generator(noise).to(dev)\n",
    "        gen_pred = discriminator(gen_features)\n",
    "\n",
    "        gen_loss = - gen_pred.mean(0).view(1)\n",
    "        gen_loss.backward()\n",
    "\n",
    "        optim_gen.step()\n",
    "\n",
    "        gen_loss = gen_loss\n",
    "        gen_losses = gen_loss.item()\n",
    "        del gen_loss\n",
    "        del noise\n",
    "        del gen_features\n",
    "        del gen_pred\n",
    "\n",
    "    #loop.set_description('epoch:{}, disc_loss:{:.4f}, gen_loss:{:.4f}'.format(epoch, disc_losses, gen_losses))\n",
    "    #loop.update(1)\n",
    "    # analyze poisson regression parameters every 20 epochs\n",
    "    if (epoch % 20 == 0):\n",
    "        with torch.no_grad():\n",
    "            generated_data = generator(Variable(torch.FloatTensor(pol_dat.shape[0], z_size).normal_()).to(dev), training=False)\n",
    "        df1 = pd.DataFrame(generated_data.data.to('cpu').numpy())\n",
    "        df1.columns = list(pol_dat)\n",
    "        df2 = back_from_dummies(df1)\n",
    "        df2['ClaimNb'] = df2['ClaimNb'].astype('float').astype('int')\n",
    "        y_gen, X_gen = dmatrices(formula,\n",
    "                                 data=df2,\n",
    "                                 return_type='dataframe')\n",
    "\n",
    "        # df2.to_csv(output_data_save_path)\n",
    "        # Fit poisson Model\n",
    "        try:\n",
    "            poisson_mod_gen = sm.GLM(y_gen, X_gen, family=sm.families.Poisson(), offset=np.log(df2['Exposure'])).fit()\n",
    "        except ValueError:\n",
    "            continue\n",
    "        # Calculate Errors\n",
    "        errors_pois = poisson_mod_gen.predict(X_test) - real_pois_preds\n",
    "\n",
    "        pois_metric.append(round(np.mean(errors_pois), 4))\n",
    "\n",
    "        if (epoch > 3):\n",
    "            plt.subplot(311)\n",
    "            plt.plot(pois_metric, label='train')\n",
    "            plt.ylabel('poission Dif')\n",
    "            plt.show()\n",
    "            plt.clf()\n",
    "\n",
    "        print('Mean Absolute Difference Pois:', round(np.mean(errors_pois), 2))\n",
    "\n",
    "        del errors_pois\n",
    "        del poisson_mod_gen\n",
    "        del generated_data\n",
    "        del df1\n",
    "        del df2\n",
    "        del gen_features\n",
    "\n",
    "        torch.save(generator.state_dict(), f='./saved_parameters/gen_test')\n",
    "        # print(pois_df)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, D-epoch 0\n",
      "c1c2\n",
      "c1c2\n",
      "c1c2\n",
      "c1c2\n",
      "c1c2\n",
      "Epoch: 0, D-epoch 1\n",
      "c1c2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[80], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m d_epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(disc_epochs):\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEpoch: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, D-epoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00md_epoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 5\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m c1, c2 \u001B[38;5;129;01min\u001B[39;00m auto_loader:  \u001B[38;5;66;03m# c1 is continous variables and c2 is the categorical variables\u001B[39;00m\n\u001B[0;32m      6\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mc1c2\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      7\u001B[0m         batch \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([c2, c1], \u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32m~\\.conda\\envs\\thesis10\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:521\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    519\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    520\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()\n\u001B[1;32m--> 521\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    522\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    523\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    524\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    525\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\.conda\\envs\\thesis10\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:561\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    559\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    560\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 561\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    562\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    563\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data)\n",
      "File \u001B[1;32m~\\.conda\\envs\\thesis10\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfetch\u001B[39m(\u001B[38;5;28mself\u001B[39m, possibly_batched_index):\n\u001B[0;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[1;32m---> 49\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     51\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32m~\\.conda\\envs\\thesis10\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfetch\u001B[39m(\u001B[38;5;28mself\u001B[39m, possibly_batched_index):\n\u001B[0;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[1;32m---> 49\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     51\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\thesis2\\Functions\\MC_WGAN_GP\\gan_scripts\\auto_loader.py:21\u001B[0m, in \u001B[0;36mPolicyDataset.__getitem__\u001B[1;34m(self, index)\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m,index):\n\u001B[0;32m     20\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcont_locs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m---> 21\u001B[0m       \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_numpy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miloc\u001B[49m\u001B[43m[\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[0;32m     22\u001B[0m               torch\u001B[38;5;241m.\u001B[39mfrom_numpy(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcont\u001B[38;5;241m.\u001B[39miloc[index]\u001B[38;5;241m.\u001B[39mvalues)\u001B[38;5;241m.\u001B[39mfloat()]\n\u001B[0;32m     23\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     24\u001B[0m       \u001B[38;5;28;01mreturn\u001B[39;00m [torch\u001B[38;5;241m.\u001B[39mfrom_numpy(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy\u001B[38;5;241m.\u001B[39miloc[index]\u001B[38;5;241m.\u001B[39mvalues)\u001B[38;5;241m.\u001B[39mfloat(),\n\u001B[0;32m     25\u001B[0m               \u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "#loop = tqdm(total=epochs, position=0, leave=False)\n",
    "for epoch in range(epochs):\n",
    "    for d_epoch in range(disc_epochs):\n",
    "        print(f'Epoch: {epoch}, D-epoch {d_epoch}')\n",
    "        for c1, c2 in auto_loader:  # c1 is continous variables and c2 is the categorical variables\n",
    "            print('c1c2')\n",
    "            batch = torch.cat([c2, c1], 1)\n",
    "            batch = batch.to(dev)\n",
    "            optim_disc.zero_grad()\n",
    "\n",
    "            # train discriminator with real data\n",
    "            real_features = Variable(batch).to(dev)\n",
    "            real_pred = discriminator(real_features)\n",
    "            # the disc outputs high numbers if it thinks the data is real, we take the negative of this\n",
    "            # Because we are minimizing loss\n",
    "            real_loss = -real_pred.mean(0).view(1)\n",
    "            real_loss.backward()\n",
    "\n",
    "            # then train the discriminator only with fake data\n",
    "            noise = Variable(torch.FloatTensor(len(batch), z_size).normal_()).to(dev)\n",
    "            fake_features = generator(noise, training=True)\n",
    "            fake_features = fake_features.detach().to(dev)  # do not propagate to the generator\n",
    "            fake_pred = discriminator(fake_features)\n",
    "            fake_loss = fake_pred.mean(0).view(1)\n",
    "            fake_loss.backward()\n",
    "\n",
    "            # this is the magic from WGAN-GP\n",
    "            gradient_penalty = calculate_gradient_penalty(discriminator.to(dev), penalty, real_features.to(dev), fake_features.to(dev))\n",
    "            gradient_penalty.backward()\n",
    "\n",
    "            # finally update the discriminator weights\n",
    "            optim_disc.step()\n",
    "\n",
    "            disc_loss = real_loss + fake_loss + gradient_penalty\n",
    "            disc_losses = disc_loss.item()\n",
    "            # Delete to prevent memory leakage\n",
    "            del gradient_penalty\n",
    "            del fake_loss\n",
    "            del real_loss\n",
    "            del disc_loss\n",
    "            del real_features\n",
    "            del real_pred\n",
    "            del noise\n",
    "            del fake_features\n",
    "            del fake_pred\n",
    "\n",
    "    for g_epoch in range(gen_epochs):\n",
    "        print(f'Epoch: {epoch}, D-epoch {g_epoch}')\n",
    "        optim_gen.zero_grad()\n",
    "\n",
    "        noise = Variable(torch.FloatTensor(len(batch), z_size).normal_()).to(dev)\n",
    "        gen_features = generator(noise).to(dev)\n",
    "        gen_pred = discriminator(gen_features)\n",
    "\n",
    "        gen_loss = - gen_pred.mean(0).view(1)\n",
    "        gen_loss.backward()\n",
    "\n",
    "        optim_gen.step()\n",
    "\n",
    "        gen_loss = gen_loss\n",
    "        gen_losses = gen_loss.item()\n",
    "        del gen_loss\n",
    "        del noise\n",
    "        del gen_features\n",
    "        del gen_pred\n",
    "\n",
    "    #loop.set_description('epoch:{}, disc_loss:{:.4f}, gen_loss:{:.4f}'.format(epoch, disc_losses, gen_losses))\n",
    "    #loop.update(1)\n",
    "    # analyze poisson regression parameters every 20 epochs\n",
    "    if (epoch % 20 == 0):\n",
    "        with torch.no_grad():\n",
    "            generated_data = generator(Variable(torch.FloatTensor(pol_dat.shape[0], z_size).normal_()).to(dev), training=False)\n",
    "        df1 = pd.DataFrame(generated_data.data.to('cpu').numpy())\n",
    "        df1.columns = list(pol_dat)\n",
    "        df2 = back_from_dummies(df1)\n",
    "        df2['ClaimNb'] = df2['ClaimNb'].astype('float').astype('int')\n",
    "        y_gen, X_gen = dmatrices(formula,\n",
    "                                 data=df2,\n",
    "                                 return_type='dataframe')\n",
    "\n",
    "        # df2.to_csv(output_data_save_path)\n",
    "        # Fit poisson Model\n",
    "        try:\n",
    "            poisson_mod_gen = sm.GLM(y_gen, X_gen, family=sm.families.Poisson(), offset=np.log(df2['Exposure'])).fit()\n",
    "        except ValueError:\n",
    "            continue\n",
    "        # Calculate Errors\n",
    "        errors_pois = poisson_mod_gen.predict(X_test) - real_pois_preds\n",
    "\n",
    "        pois_metric.append(round(np.mean(errors_pois), 4))\n",
    "\n",
    "        if (epoch > 3):\n",
    "            plt.subplot(311)\n",
    "            plt.plot(pois_metric, label='train')\n",
    "            plt.ylabel('poission Dif')\n",
    "            plt.show()\n",
    "            plt.clf()\n",
    "\n",
    "        print('Mean Absolute Difference Pois:', round(np.mean(errors_pois), 2))\n",
    "\n",
    "        del errors_pois\n",
    "        del poisson_mod_gen\n",
    "        del generated_data\n",
    "        del df1\n",
    "        del df2\n",
    "        del gen_features\n",
    "\n",
    "        torch.save(generator.state_dict(), f='./saved_parameters/gen_test')\n",
    "        # print(pois_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "This script runs the multi GAN and allows you to step through each part\n",
    "# divide y by exposure in xpxixpy\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "# import modules\n",
    "import torch.optim as optim\n",
    "from patsy import dmatrices\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from Functions import dataprep as prep\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "import math\n",
    "import joblib\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "## Import created modules\n",
    "from Functions.MC_WGAN_GP.gan_scripts.auto_loader import PolicyDataset\n",
    "from Functions.MC_WGAN_GP.gan_scripts.generator2_v2 import Generator2\n",
    "from Functions.MC_WGAN_GP.gan_scripts.discriminator2_v3 import Discriminator2\n",
    "from Functions.MC_WGAN_GP.gan_scripts.gradiant_penalty import calculate_gradient_penalty\n",
    "from Functions.MC_WGAN_GP.gan_scripts.undo_dummy import back_from_dummies\n",
    "from Functions import metrics\n",
    "\n",
    "from torch.autograd.variable import Variable\n",
    "\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    devid = torch.cuda.current_device()\n",
    "    torch.cuda.set_device(devid)\n",
    "    dev = torch.device(\"cuda\")\n",
    "else:\n",
    "    dev = torch.device(\"cpu\")\n",
    "    \n",
    "print(f'Device: {dev}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle(\"./data/gan_dataprep/train_gan.pickle\")\n",
    "val = pd.read_pickle(\"./data/gan_dataprep/val_gan.pickle\")\n",
    "ss = joblib.load('./data/gan_dataprep/scaler.pickle')\n",
    "pol_dat = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def glm_metrics(train, val):\n",
    "    # Transform back\n",
    "    train = pd.DataFrame(ss.inverse_transform(train), columns = train.columns)\n",
    "    val = pd.DataFrame(ss.inverse_transform(val), columns = val.columns)\n",
    "        \n",
    "    train = back_from_dummies(train)\n",
    "    val = back_from_dummies(val)\n",
    "    \n",
    "    train['ClaimNb'] = train['ClaimNb'].astype(float).astype(int)\n",
    "    val['ClaimNb'] = val['ClaimNb'].astype(float).astype(int)\n",
    "    \n",
    "    train['Density'] = train['Density'].clip(lower=1)\n",
    "\n",
    "    glm_train = prep.data_cleaning_frequency_schelldorfer(train)\n",
    "    glm_val = prep.data_cleaning_frequency_schelldorfer(val)\n",
    "\n",
    "    # Train GLM\n",
    "    formula1 = \"ClaimNb ~ VehPowerGLM + C(VehAgeGLM, Treatment(reference=2)) + C(DrivAgeGLM, Treatment(reference=5)) + BonusMalusGLM + VehBrand + VehGas + DensityGLM + C(Region, Treatment(reference='R24')) + AreaGLM\"\n",
    "    glm1 = smf.glm(formula=formula1, data=glm_train, family=sm.families.Poisson(link=sm.families.links.log()), offset=np.log(glm_train['Exposure'])).fit()\n",
    "    mets = metrics.poisson_deviance(glm1.predict(glm_val, offset=np.log(glm_val['Exposure'])), glm_val['ClaimNb'])\n",
    "    \n",
    "    return mets\n",
    "\n",
    "def xpxixpy(X, y):\n",
    "    return np.dot(np.linalg.inv(np.dot(X.T, X)), np.dot(X.T, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.02011267896118"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glm_metrics(train, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "output_size = [1, 1, 1, 1, 1, 1, 5, 11, 2, 22, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:17733, disc_loss:9109.8837890625, gen_loss:2900969.25:  89%|████████▊ | 17734/20000 [11:46:06<1:31:47,  2.43s/it]         \n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # %%\n",
    "# formula = 'ClaimNb ~ VehBrand + VehGas + Region + AreaGLM + VehPower + VehAge + DrivAge + DensityGLM + BonusMalus'\n",
    "# # %%\n",
    "# # Wrangle train data\n",
    "# td = back_from_dummies(train)\n",
    "# td['ClaimNb'] = td['ClaimNb'].astype('float').astype('int')\n",
    "# y_real, X_real = dmatrices(formula,\n",
    "#                            data=td,\n",
    "#                            return_type='dataframe')\n",
    "\n",
    "\n",
    "# xy = xpxixpy(X_real, y_real)\n",
    "# disc_add_rows = xy.shape[0]\n",
    "\n",
    "# # Fit a poisson Model\n",
    "# poisson_mod = sm.GLM(y_real, X_real, family=sm.families.Poisson(), offset=td['Exposure']).fit()\n",
    "# original_params = poisson_mod.params\n",
    "\n",
    "# lower = poisson_mod.params - 1.96 * poisson_mod.bse\n",
    "# upper = poisson_mod.params + 1.96 * poisson_mod.bse\n",
    "\n",
    "# # Wrangle Test Data\n",
    "# test2 = back_from_dummies(val)\n",
    "# test2['ClaimNb'] = test2['ClaimNb'].astype('float').astype('int')\n",
    "# y_test, X_test = dmatrices(formula,\n",
    "#                            data=test2,\n",
    "#                            return_type='dataframe')\n",
    "# y_test_resp = np.squeeze(y_test) / np.squeeze(test2['Exposure'])\n",
    "\n",
    "# # make predictions on test data with models trained on train data\n",
    "# dev_real = np.mean(abs(poisson_mod.predict(X_test) - y_test))\n",
    "\n",
    "# # %%\n",
    "# \"\"\"\n",
    "# This next section contains everything that we can tune in the GAN\n",
    "# \"\"\"\n",
    "\n",
    "# Information about the size of the data\n",
    "data_size = pol_dat.shape[1]  # number of cols in pol_dat\n",
    "var_locs = [0, 1, 2, 3, 4, 5]  # tells us where the continous variables are\n",
    "\n",
    "# parameters\n",
    "z_size = 100  # how big is the random vector fed into the generator\n",
    "# we should only need the 55?\n",
    "batch_size = 7000\n",
    "temperature = None  # comes into play with the categorical activation see multioutput.py\n",
    "\n",
    "# Generator tuning\n",
    "gen_hidden_sizes = [100, 100, 100]\n",
    "gen_bn_decay = .90\n",
    "gen_l2_regularization = 0\n",
    "gen_learning_rate = 0.01\n",
    "noise_size = z_size\n",
    "\n",
    "assert sum(output_size) == data_size\n",
    "\n",
    "# Discriminator tuning\n",
    "disc_hidden_sizes = [data_size, data_size]\n",
    "disc_bn_decay = .90\n",
    "critic_bool = True  # if false then between 0 and 1\n",
    "mini_batch_bool = False\n",
    "disc_leaky_param = 0.2\n",
    "disc_l2_regularization = 0\n",
    "disc_learning_rate = 0.01\n",
    "penalty = 10  ## deals with gradiant penalty\n",
    "\n",
    "auto_data = PolicyDataset(pol_dat, var_locs)\n",
    "# auto_loader = DataLoader(auto_data, persistent_workers=True,\n",
    "#                          batch_size=batch_size,\n",
    "#                          pin_memory=True,\n",
    "#                          shuffle=True,\n",
    "#                          num_workers=7\n",
    "#                          )\n",
    "\n",
    "# initilize generator and discriminator\n",
    "generator = Generator2(\n",
    "    noise_size=noise_size,\n",
    "    output_size=output_size,\n",
    "    hidden_sizes=gen_hidden_sizes,\n",
    "    bn_decay=gen_bn_decay\n",
    ").to(dev)\n",
    "\n",
    "discriminator = Discriminator2(\n",
    "    input_size=data_size,\n",
    "    hidden_sizes=disc_hidden_sizes,\n",
    "    bn_decay=disc_bn_decay,  # no batch normalization for the critic\n",
    "    critic=critic_bool,  # Do you want a critic\n",
    "    leaky_param=disc_leaky_param,  # parameter for leakyRelu\n",
    "    mini_batch=mini_batch_bool,  # Do you want any mini batch extras\n",
    "    add_rows=56  # Number of rows to add if appending extra rows\n",
    ").to(dev)\n",
    "\n",
    "generator = generator.to(dev)\n",
    "discriminator = discriminator.to(dev)\n",
    "\n",
    "optim_gen = optim.Adam(generator.parameters(),\n",
    "                       weight_decay=gen_l2_regularization,\n",
    "                       lr=gen_learning_rate\n",
    "                       )\n",
    "\n",
    "optim_disc = optim.Adam(discriminator.parameters(),\n",
    "                        weight_decay=disc_l2_regularization,\n",
    "                        lr=disc_learning_rate\n",
    "                        )\n",
    "\n",
    "epochs = 20000\n",
    "disc_epochs = 2\n",
    "gen_epochs = 1\n",
    "generator.train(mode=True)\n",
    "discriminator.train(mode=True)\n",
    "disc_losses = []\n",
    "gen_losses = []\n",
    "pois_metric = []\n",
    "# %%\n",
    "disc_loss = torch.tensor(9999)\n",
    "gen_loss = torch.tensor(9999)\n",
    "loop = tqdm(total=epochs, position=0, leave=False)\n",
    "\n",
    "generator = generator.to(dev)\n",
    "discriminator = discriminator.to(dev)\n",
    "\n",
    "cats, conts = auto_data.get_catcont()\n",
    "batch = torch.cat([conts, cats], 1)\n",
    "real_features = Variable(batch)\n",
    "real_features = real_features.to(dev)\n",
    "featlen = len(real_features)\n",
    "\n",
    "shuffled_features = real_features\n",
    "for epoch in range(epochs):\n",
    "    # Shuffling of features\n",
    "    rand_indx = torch.randperm(featlen)\n",
    "    shuffled_features = shuffled_features[rand_indx]\n",
    "\n",
    "    # For each batch\n",
    "    for real_batch in torch.split(shuffled_features, batch_size):\n",
    "        real_batch_size = len(real_batch)\n",
    "\n",
    "        # Discriminator\n",
    "        optim_disc.zero_grad()\n",
    "\n",
    "        # train discriminator with real data\n",
    "        real_pred = discriminator(real_batch)\n",
    "        # the disc outputs high numbers if it thinks the data is real, we take the negative of this\n",
    "        # Because we are minimizing loss\n",
    "        real_loss = -real_pred.mean(0).view(1)\n",
    "        real_loss.backward()\n",
    "\n",
    "        # then train the discriminator only with fake data\n",
    "        noise = Variable(torch.FloatTensor(real_batch_size, z_size).normal_()).to(dev)\n",
    "        fake_features = generator(noise, training=True)\n",
    "        fake_features = fake_features.detach().to(dev)  # do not propagate to the generator\n",
    "        fake_pred = discriminator(fake_features)\n",
    "        fake_loss = fake_pred.mean(0).view(1)\n",
    "        fake_loss.backward()\n",
    "\n",
    "        # this is the magic from WGAN-GP\n",
    "        gradient_penalty = calculate_gradient_penalty(discriminator.to(dev), penalty, real_batch.to(dev), fake_features.to(dev))\n",
    "        gradient_penalty.backward()\n",
    "\n",
    "        # finally update the discriminator weights\n",
    "        optim_disc.step()\n",
    "\n",
    "        disc_loss = real_loss + fake_loss + gradient_penalty\n",
    "        disc_losses = disc_loss.item()\n",
    "        # Delete to prevent memory leakage\n",
    "        del real_pred\n",
    "        del real_loss\n",
    "        del noise\n",
    "        del fake_features\n",
    "        del fake_pred\n",
    "        del fake_loss\n",
    "        del gradient_penalty\n",
    "        del disc_loss\n",
    "\n",
    "        optim_gen.zero_grad()\n",
    "\n",
    "        noise = Variable(torch.FloatTensor(real_batch_size, z_size).normal_()).to(dev)\n",
    "        gen_features = generator(noise)\n",
    "        gen_features = gen_features.to(dev)\n",
    "        gen_pred = discriminator(gen_features)\n",
    "\n",
    "        gen_loss = - gen_pred.mean(0).view(1)\n",
    "        gen_loss.backward()\n",
    "\n",
    "        optim_gen.step()\n",
    "\n",
    "        gen_loss = gen_loss\n",
    "        gen_losses = gen_loss.item()\n",
    "        del noise\n",
    "        del gen_features\n",
    "        del gen_pred\n",
    "        del gen_loss\n",
    "\n",
    "    loop.set_description(f'epoch:{epoch}, disc_loss:{disc_losses}, gen_loss:{gen_losses}')\n",
    "    loop.update(1)\n",
    "\n",
    "    # analyze poisson regression parameters every 20 epochs\n",
    "    if False & (epoch % 20 == 0) & (epoch > 10):\n",
    "        with torch.no_grad():\n",
    "            generated_data = generator(Variable(torch.FloatTensor(pol_dat.shape[0], z_size).normal_()).to(dev), training=False)\n",
    "        df1 = pd.DataFrame(generated_data.data.to('cpu').numpy().round(2))\n",
    "        df1.columns = list(pol_dat)\n",
    "        \n",
    "        try:\n",
    "            dev_gen = glm_metrics(df1, val)\n",
    "            print(f'Generated Deviance: {dev_gen}')\n",
    "        except ValueError:\n",
    "            print('No value found yet')\n",
    "        \n",
    "        #print(f'Real poisson prediction: {dev_real}')\n",
    "        #print(f'Generative poisson prediction: {dev_gen}')\n",
    "        #, real_poiss: {dev_real}, gen_poiss: {dev_gen}')\n",
    "\n",
    "        if (epoch > 3):\n",
    "            plt.plot(dev_gen, label='train')\n",
    "            plt.ylabel('poission Dif')\n",
    "            plt.show()\n",
    "            plt.clf()\n",
    "\n",
    "        del df1\n",
    "\n",
    "        #torch.save(generator.state_dict(), f='./saved_parameters/gen_test')\n",
    "        # print(pois_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(generated_data.data.to('cpu').numpy().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(ss.inverse_transform(df1), columns = train.columns)\n",
    "df2 = back_from_dummies(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2['ClaimNb'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2901202.5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform back\n",
    "train = pd.DataFrame(ss.inverse_transform(df1), columns = train.columns)\n",
    "train = back_from_dummies(train)\n",
    "train['ClaimNb'] = train['ClaimNb'].astype(float).astype(int)\n",
    "train['Density'] = train['Density'].clip(lower=1)\n",
    "glm_train = prep.data_cleaning_frequency_schelldorfer(train)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

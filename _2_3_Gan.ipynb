{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec5811cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "This script runs the multi GAN and allows you to step through each part\n",
    "# divide y by exposure in xpxixpy\n",
    "\"\"\"\n",
    "\n",
    "# import modules\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd.variable import Variable\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from patsy import dmatrices\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "## Import created modules\n",
    "from Functions.MC_WGAN_GP.gan_scripts.auto_loader import PolicyDataset\n",
    "from Functions.MC_WGAN_GP.gan_scripts.generator2_v2 import Generator2\n",
    "from Functions.MC_WGAN_GP.gan_scripts.discriminator2_v3 import Discriminator2\n",
    "from Functions.MC_WGAN_GP.gan_scripts.gradiant_penalty import calculate_gradient_penalty\n",
    "from Functions.MC_WGAN_GP.gan_scripts.undo_dummy import back_from_dummies\n",
    "\n",
    "from torch.autograd.variable import Variable\n",
    "from torch.optim import Adam\n",
    "\n",
    "from multi_categorical_gans.datasets.dataset import Dataset\n",
    "from multi_categorical_gans.datasets.formats import data_formats, loaders\n",
    "\n",
    "from multi_categorical_gans.methods.general.discriminator import Discriminator\n",
    "from multi_categorical_gans.methods.general.generator import Generator\n",
    "from multi_categorical_gans.methods.general.multi_categorical import MultiCategorical\n",
    "from multi_categorical_gans.methods.general.wgan_gp import calculate_gradient_penalty\n",
    "\n",
    "from multi_categorical_gans.utils.categorical import load_variable_sizes_from_metadata\n",
    "from multi_categorical_gans.utils.commandline import DelayedKeyboardInterrupt, parse_int_list\n",
    "from multi_categorical_gans.utils.cuda import to_cuda_if_available, to_cpu_if_available\n",
    "from multi_categorical_gans.utils.initialization import load_or_initialize\n",
    "from multi_categorical_gans.utils.logger import Logger\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71aeb2f8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_pickle(\"./data/gan_dataprep/train_gan.pickle\")\n",
    "val = pd.read_pickle(\"./data/gan_dataprep/val_gan.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8206152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "487648"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f64805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_dat = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "964fa3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'ClaimNb ~ VehBrand + VehGas + Region + AreaGLM + VehPower + VehAge + DrivAge + DensityGLM + BonusMalus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "befae7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrangle train data\n",
    "td = back_from_dummies(train)\n",
    "td['ClaimNb'] = td['ClaimNb'].astype('float').astype('int')\n",
    "y_real, X_real = dmatrices(formula,\n",
    "                 data=td,\n",
    "                 return_type='dataframe')\n",
    "def xpxixpy(X,y):\n",
    "            return np.dot(np.linalg.inv(np.dot(X.T,X)), np.dot(X.T,y))\n",
    "xy = xpxixpy(X_real,y_real)\n",
    "disc_add_rows = xy.shape[0]\n",
    "\n",
    "# Fit a poisson Model\n",
    "poisson_mod = sm.GLM(y_real,X_real,family = sm.families.Poisson(), offset = td['Exposure']).fit()\n",
    "original_params = poisson_mod.params\n",
    "\n",
    "lower = poisson_mod.params - 1.96*poisson_mod.bse  \n",
    "upper = poisson_mod.params + 1.96*poisson_mod.bse \n",
    "\n",
    "# Wrangle Test Data\n",
    "test2 = back_from_dummies(val)\n",
    "test2['ClaimNb'] = test2['ClaimNb'].astype('float').astype('int')\n",
    "y_test, X_test = dmatrices(formula,\n",
    "                 data=test2,\n",
    "                 return_type='dataframe')\n",
    "y_test_resp = np.squeeze(y_test)/np.squeeze(test2['Exposure'])\n",
    "\n",
    "# make predictions on test data with models trained on train data\n",
    "real_pois_preds = poisson_mod.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2ad38ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This next section contains everything that we can tune in the GAN\n",
    "\"\"\"\n",
    "\n",
    "# Information about the size of the data\n",
    "data_size = pol_dat.shape[1] # number of cols in pol_dat \n",
    "var_locs = [0,1,2,3,4,5] # tells us where the continous variables are \n",
    "\n",
    "\n",
    "# parameters\n",
    "z_size = 100 # how big is the random vector fed into the generator\n",
    "# we should only need the 55?\n",
    "batch_size = 1500\n",
    "temperature = None # comes into play with the categorical activation see multioutput.py\n",
    "\n",
    "# Generator tuning\n",
    "gen_hidden_sizes = [100,100,100]\n",
    "gen_bn_decay = .25\n",
    "gen_l2_regularization = 0.1\n",
    "gen_learning_rate = 0.001\n",
    "noise_size = z_size\n",
    "output_size = [1,1,1,1,1,1,5,11,2,22,6]  # how many categories with in each variable\n",
    "\n",
    "assert sum(output_size) == data_size\n",
    "\n",
    "# Discriminator tuning\n",
    "disc_hidden_sizes = [data_size,data_size]\n",
    "disc_bn_decay = .2\n",
    "critic_bool = True # if false then between 0 and 1\n",
    "mini_batch_bool = False\n",
    "disc_leaky_param = 0.2\n",
    "disc_l2_regularization = 0.0\n",
    "disc_learning_rate = 0.001\n",
    "penalty = 1 ## deals with gradiant penalty\n",
    "\n",
    "auto_data = PolicyDataset(pol_dat, var_locs)\n",
    "auto_loader = DataLoader(auto_data,\n",
    "                         batch_size = batch_size,\n",
    "                         pin_memory = True, \n",
    "                         shuffle = True\n",
    "                         )\n",
    "\n",
    "# initilize generator and discriminator\n",
    "generator = Generator2(\n",
    "    noise_size = noise_size,\n",
    "    output_size =  output_size,\n",
    "    hidden_sizes = gen_hidden_sizes,\n",
    "    bn_decay = gen_bn_decay\n",
    "    )\n",
    "\n",
    "discriminator = Discriminator2(\n",
    "    input_size = data_size,\n",
    "    hidden_sizes= disc_hidden_sizes,\n",
    "    bn_decay = disc_bn_decay,               # no batch normalization for the critic\n",
    "    critic = critic_bool,                   # Do you want a critic\n",
    "    leaky_param = disc_leaky_param,         # parameter for leakyRelu\n",
    "    mini_batch = mini_batch_bool,           # Do you want any mini batch extras\n",
    "    add_rows = disc_add_rows # Number of rows to add if appending extra rows \n",
    "    )\n",
    "\n",
    "\n",
    "optim_gen = optim.Adam(generator.parameters(),\n",
    "                       weight_decay= gen_l2_regularization,\n",
    "                       lr= gen_learning_rate\n",
    "                       )\n",
    "\n",
    "optim_disc = optim.Adam(discriminator.parameters(),\n",
    "                        weight_decay= disc_l2_regularization,\n",
    "                        lr= disc_learning_rate\n",
    "                        )    \n",
    "\n",
    "epochs = 1000\n",
    "disc_epochs = 2\n",
    "gen_epochs = 1\n",
    "generator.train(mode=True)\n",
    "discriminator.train(mode=True)\n",
    "disc_losses = []\n",
    "gen_losses = []\n",
    "pois_metric = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d030eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:0, disc_loss:1043.2046, gen_loss:-3.6720:   0%|          | 1/1000 [00:56<15:42:30, 56.61s/it]/Users/janjaniszewski/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arraylike.py:396: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "epoch:4, disc_loss:2557.7627, gen_loss:-4.0181:   0%|          | 4/1000 [04:38<15:27:03, 55.85s/it]"
     ]
    }
   ],
   "source": [
    "disc_loss = torch.tensor(9999)\n",
    "gen_loss = torch.tensor(9999)\n",
    "loop = tqdm(total = epochs, position = 0, leave = False)\n",
    "for epoch in range(epochs):\n",
    "    for d_epoch in range(disc_epochs):  \n",
    "        for c1,c2 in auto_loader: # c1 is continous variables and c2 is the categorical variables\n",
    "            batch = torch.cat([c2,c1],1) \n",
    "            optim_disc.zero_grad()\n",
    "            \n",
    "            # train discriminator with real data\n",
    "            real_features = Variable(batch)\n",
    "            real_pred = discriminator(real_features)\n",
    "            # the disc outputs high numbers if it thinks the data is real, we take the negative of this\n",
    "            # Because we are minimizing loss\n",
    "            real_loss = -real_pred.mean(0).view(1)  \n",
    "            real_loss.backward()\n",
    "\n",
    "            # then train the discriminator only with fake data\n",
    "            noise = Variable(torch.FloatTensor(len(batch), z_size).normal_())\n",
    "            fake_features = generator(noise, training = True)\n",
    "            fake_features = fake_features.detach()  # do not propagate to the generator\n",
    "            fake_pred = discriminator(fake_features)\n",
    "            fake_loss = fake_pred.mean(0).view(1)\n",
    "            fake_loss.backward()\n",
    "            \n",
    "            # this is the magic from WGAN-GP\n",
    "            gradient_penalty = calculate_gradient_penalty(discriminator, penalty, real_features, fake_features)\n",
    "            gradient_penalty.backward()\n",
    "\n",
    "            # finally update the discriminator weights\n",
    "            optim_disc.step()\n",
    "\n",
    "            disc_loss = real_loss + fake_loss + gradient_penalty\n",
    "            disc_losses = disc_loss.item()\n",
    "            # Delete to prevent memory leakage\n",
    "            del gradient_penalty\n",
    "            del fake_loss\n",
    "            del real_loss\n",
    "            del disc_loss\n",
    "            del real_features\n",
    "            del real_pred\n",
    "            del noise\n",
    "            del fake_features\n",
    "            del fake_pred\n",
    "    \n",
    "\n",
    "    for g_epoch in range(gen_epochs):\n",
    "        optim_gen.zero_grad()\n",
    "\n",
    "        noise = Variable(torch.FloatTensor(len(batch), z_size).normal_())\n",
    "        gen_features = generator(noise)\n",
    "        gen_pred = discriminator(gen_features)\n",
    "\n",
    "        gen_loss = - gen_pred.mean(0).view(1)\n",
    "        gen_loss.backward()\n",
    "\n",
    "        optim_gen.step()\n",
    "\n",
    "        gen_loss = gen_loss\n",
    "        gen_losses = gen_loss.item()\n",
    "        del gen_loss \n",
    "        del noise\n",
    "        del gen_features\n",
    "        del gen_pred\n",
    "            \n",
    "    loop.set_description('epoch:{}, disc_loss:{:.4f}, gen_loss:{:.4f}'.format(epoch, disc_losses, gen_losses))\n",
    "    loop.update(1) \n",
    "    # analyze poisson regression parameters every 20 epochs\n",
    "    if(epoch % 20 == 0):\n",
    "        with torch.no_grad():\n",
    "            generated_data = generator(Variable(torch.FloatTensor(pol_dat.shape[0], z_size).normal_()), training = False)\n",
    "        df1 = pd.DataFrame(generated_data.data.numpy())\n",
    "        df1.columns = list(pol_dat)\n",
    "        df2 = back_from_dummies(df1)\n",
    "        df2['ClaimNb'] = df2['ClaimNb'].astype('float').astype('int')\n",
    "        y_gen, X_gen = dmatrices(formula,\n",
    "                             data=df2,\n",
    "                             return_type='dataframe')\n",
    "        \n",
    "        #df2.to_csv(output_data_save_path)\n",
    "        # Fit poisson Model\n",
    "        try:\n",
    "            poisson_mod_gen = sm.GLM(y_gen,X_gen,family = sm.families.Poisson(), offset = np.log(df2['Exposure'])).fit()\n",
    "        except ValueError:\n",
    "            continue\n",
    "        # Calculate Errors\n",
    "        errors_pois = poisson_mod_gen.predict(X_test) - real_pois_preds\n",
    "        \n",
    "        pois_metric.append(round(np.mean(errors_pois), 4))\n",
    "        \n",
    "        if(epoch > 3) :\n",
    "            plt.subplot(311)\n",
    "            plt.plot(pois_metric, label = 'train')\n",
    "            plt.ylabel('poission Dif')\n",
    "            plt.show()\n",
    "            plt.clf()\n",
    "        \n",
    "        print('Mean Absolute Difference Pois:', round(np.mean(errors_pois), 2))\n",
    "        \n",
    "        del errors_pois\n",
    "        del y_gen2\n",
    "        del poisson_mod_gen\n",
    "        del y_gen\n",
    "        del X_gen\n",
    "        del importances_gen\n",
    "        del gen_predictions \n",
    "        del generated_data\n",
    "        del df1\n",
    "        del df2\n",
    "        del gen_features\n",
    "        \n",
    "        \n",
    "        torch.save(generator.state_dict(), f='./saved_parameters/gen_test')\n",
    "        print(pois_df)\n",
    "\n",
    "      \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

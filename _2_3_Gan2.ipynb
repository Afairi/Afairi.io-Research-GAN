{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "This script runs the multi GAN and allows you to step through each part\n",
    "# divide y by exposure in xpxixpy\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "# import modules\n",
    "import torch.optim as optim\n",
    "from patsy import dmatrices\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from Functions import dataprep as prep\n",
    "from logging import Logger\n",
    "from torch.optim import Adam\n",
    "from Functions.mcwgan.functions import to_cuda_if_available, to_cpu_if_available, DelayedKeyboardInterrupt, Logger, Dataset\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "import math\n",
    "import joblib\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "## Import created modules\n",
    "from Functions.MC_WGAN_GP.gan_scripts.auto_loader import PolicyDataset\n",
    "from Functions.MC_WGAN_GP.gan_scripts.generator2_v2 import Generator2\n",
    "from Functions.MC_WGAN_GP.gan_scripts.discriminator2_v3 import Discriminator2\n",
    "from Functions.MC_WGAN_GP.gan_scripts.gradiant_penalty import calculate_gradient_penalty\n",
    "from Functions.MC_WGAN_GP.gan_scripts.undo_dummy import back_from_dummies\n",
    "from Functions import metrics\n",
    "\n",
    "from torch.autograd.variable import Variable\n",
    "\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    devid = torch.cuda.current_device()\n",
    "    torch.cuda.set_device(devid)\n",
    "    dev = torch.device(\"cuda\")\n",
    "else:\n",
    "    dev = torch.device(\"cpu\")\n",
    "    \n",
    "print(f'Device: {dev}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_pickle(\"./data/gan_dataprep/train_gan.pickle\")\n",
    "val = pd.read_pickle(\"./data/gan_dataprep/val_gan.pickle\")\n",
    "ss = joblib.load('./data/gan_dataprep/scaler.pickle')\n",
    "pol_dat = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train = Dataset(train)\n",
    "val = Dataset(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def glm_metrics(train, val):\n",
    "    # Transform back\n",
    "    train = pd.DataFrame(ss.inverse_transform(train), columns = train.columns)\n",
    "    val = pd.DataFrame(ss.inverse_transform(val), columns = val.columns)\n",
    "        \n",
    "    train = back_from_dummies(train)\n",
    "    val = back_from_dummies(val)\n",
    "    \n",
    "    train['ClaimNb'] = train['ClaimNb'].astype(float).astype(int)\n",
    "    val['ClaimNb'] = val['ClaimNb'].astype(float).astype(int)\n",
    "    \n",
    "    train['Density'] = train['Density'].clip(lower=1)\n",
    "\n",
    "    glm_train = prep.data_cleaning_frequency_schelldorfer(train)\n",
    "    glm_val = prep.data_cleaning_frequency_schelldorfer(val)\n",
    "\n",
    "    # Train GLM\n",
    "    formula1 = \"ClaimNb ~ VehPowerGLM + C(VehAgeGLM, Treatment(reference=2)) + C(DrivAgeGLM, Treatment(reference=5)) + BonusMalusGLM + VehBrand + VehGas + DensityGLM + C(Region, Treatment(reference='R24')) + AreaGLM\"\n",
    "    glm1 = smf.glm(formula=formula1, data=glm_train, family=sm.families.Poisson(link=sm.families.links.log()), offset=np.log(glm_train['Exposure'])).fit()\n",
    "    mets = metrics.poisson_deviance(glm1.predict(glm_val, offset=np.log(glm_val['Exposure'])), glm_val['ClaimNb'])\n",
    "    \n",
    "    return mets\n",
    "\n",
    "def xpxixpy(X, y):\n",
    "    return np.dot(np.linalg.inv(np.dot(X.T, X)), np.dot(X.T, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "glm_metrics(train, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "output_size = [1, 1, 1, 1, 1, 1, 5, 11, 2, 22, 6]\n",
    "# Information about the size of the data\n",
    "data_size = pol_dat.shape[1]  # number of cols in pol_dat\n",
    "var_locs = [0, 1, 2, 3, 4, 5]  # tells us where the continous variables are\n",
    "\n",
    "# parameters\n",
    "z_size = 100  # how big is the random vector fed into the generator\n",
    "# we should only need the 55?\n",
    "batch_size = 100\n",
    "temperature = None  # comes into play with the categorical activation see multioutput.py\n",
    "\n",
    "# Generator tuning\n",
    "gen_hidden_sizes = [100, 100, 100]\n",
    "gen_bn_decay = .90\n",
    "gen_l2_regularization = 0.001\n",
    "gen_learning_rate = 0.01\n",
    "noise_size = z_size\n",
    "\n",
    "assert sum(output_size) == data_size\n",
    "\n",
    "# Discriminator tuning\n",
    "disc_hidden_sizes = [data_size, data_size]\n",
    "disc_bn_decay = .90\n",
    "critic_bool = True  # if false then between 0 and 1\n",
    "mini_batch_bool = False\n",
    "disc_leaky_param = 0.2\n",
    "disc_l2_regularization = 0.001\n",
    "l2_regularization = disc_l2_regularization\n",
    "disc_learning_rate = 0.01\n",
    "penalty = 0.1  ## deals with gradiant penalty\n",
    "epochs = 20000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "auto_data = PolicyDataset(pol_dat, var_locs)\n",
    "\n",
    "# initilize generator and discriminator\n",
    "generator = Generator2(\n",
    "    noise_size=noise_size,\n",
    "    output_size=output_size,\n",
    "    hidden_sizes=gen_hidden_sizes,\n",
    "    bn_decay=gen_bn_decay\n",
    ").to(dev)\n",
    "\n",
    "discriminator = Discriminator2(\n",
    "    input_size=data_size,\n",
    "    hidden_sizes=disc_hidden_sizes,\n",
    "    bn_decay=disc_bn_decay,  # no batch normalization for the critic\n",
    "    critic=critic_bool,  # Do you want a critic\n",
    "    leaky_param=disc_leaky_param,  # parameter for leakyRelu\n",
    "    mini_batch=mini_batch_bool,  # Do you want any mini batch extras\n",
    "    add_rows=56  # Number of rows to add if appending extra rows\n",
    ").to(dev)\n",
    "\n",
    "generator = generator.to(dev)\n",
    "discriminator = discriminator.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "generator.train(mode=True)\n",
    "discriminator.train(mode=True)\n",
    "disc_losses = []\n",
    "gen_losses = []\n",
    "pois_metric = []\n",
    "# %%\n",
    "disc_loss = torch.tensor(9999)\n",
    "gen_loss = torch.tensor(9999)\n",
    "loop = tqdm(total=epochs, position=0, leave=False)\n",
    "\n",
    "generator = generator.to(dev)\n",
    "discriminator = discriminator.to(dev)\n",
    "\n",
    "cats, conts = auto_data.get_catcont()\n",
    "batch = torch.cat([conts, cats], 1)\n",
    "real_features = Variable(batch)\n",
    "real_features = real_features.to(dev)\n",
    "featlen = len(real_features)\n",
    "\n",
    "shuffled_features = real_features\n",
    "optim_disc.zero_grad()\n",
    "optim_gen.zero_grad()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Shuffling of features\n",
    "    rand_indx = torch.randperm(featlen)\n",
    "    shuffled_features = shuffled_features[rand_indx]\n",
    "\n",
    "    # For each batch\n",
    "    for real_batch in torch.split(shuffled_features, batch_size):\n",
    "        real_batch_size = len(real_batch)\n",
    "\n",
    "        # Discriminator\n",
    "\n",
    "        # train discriminator with real data\n",
    "        real_pred = discriminator(real_batch)\n",
    "        # the disc outputs high numbers if it thinks the data is real, we take the negative of this\n",
    "        # Because we are minimizing loss\n",
    "        real_loss = -real_pred.mean(0).view(1)\n",
    "        real_loss.backward()\n",
    "\n",
    "        # then train the discriminator only with fake data\n",
    "        noise = Variable(torch.FloatTensor(real_batch_size, z_size).normal_()).to(dev)\n",
    "        fake_features = generator(noise, training=True)\n",
    "        fake_features = fake_features.detach().to(dev)  # do not propagate to the generator\n",
    "        fake_pred = discriminator(fake_features)\n",
    "        fake_loss = fake_pred.mean(0).view(1)\n",
    "        fake_loss.backward()\n",
    "\n",
    "        # this is the magic from WGAN-GP\n",
    "        gradient_penalty = calculate_gradient_penalty(discriminator.to(dev), penalty, real_batch.to(dev), fake_features.to(dev))\n",
    "        gradient_penalty.backward()\n",
    "\n",
    "        # finally update the discriminator weights\n",
    "        optim_disc.step()\n",
    "\n",
    "        disc_loss = real_loss + fake_loss + gradient_penalty\n",
    "        disc_losses += [disc_loss]\n",
    "        # Delete to prevent memory leakage\n",
    "        del real_pred\n",
    "        del real_loss\n",
    "        del noise\n",
    "        del fake_features\n",
    "        del fake_pred\n",
    "        del fake_loss\n",
    "        del gradient_penalty\n",
    "        del disc_loss\n",
    "\n",
    "\n",
    "        noise = Variable(torch.FloatTensor(real_batch_size, z_size).normal_()).to(dev)\n",
    "        gen_features = generator(noise)\n",
    "        gen_features = gen_features.to(dev)\n",
    "        gen_pred = discriminator(gen_features)\n",
    "\n",
    "        gen_loss = - gen_pred.mean(0).view(1)\n",
    "        gen_loss.backward()\n",
    "\n",
    "        optim_gen.step()\n",
    "\n",
    "        gen_losses += [gen_loss]\n",
    "        del noise\n",
    "        del gen_features\n",
    "        del gen_pred\n",
    "        del gen_loss\n",
    "\n",
    "    loop.set_description(f'epoch:{epoch}, disc_loss:{disc_losses}, gen_loss:{gen_losses}')\n",
    "    loop.update(1)\n",
    "\n",
    "    # analyze poisson regression parameters every 20 epochs\n",
    "    if False & (epoch % 20 == 0) & (epoch > 10):\n",
    "        with torch.no_grad():\n",
    "            generated_data = generator(Variable(torch.FloatTensor(pol_dat.shape[0], z_size).normal_()).to(dev), training=False)\n",
    "        df1 = pd.DataFrame(generated_data.data.to('cpu').numpy().round(2))\n",
    "        df1.columns = list(pol_dat)\n",
    "        \n",
    "        try:\n",
    "            dev_gen = glm_metrics(df1, val)\n",
    "            print(f'Generated Deviance: {dev_gen}')\n",
    "        except ValueError:\n",
    "            print('No value found yet')\n",
    "        \n",
    "        #print(f'Real poisson prediction: {dev_real}')\n",
    "        #print(f'Generative poisson prediction: {dev_gen}')\n",
    "        #, real_poiss: {dev_real}, gen_poiss: {dev_gen}')\n",
    "\n",
    "        if (epoch > 3):\n",
    "            plt.plot(dev_gen, label='train')\n",
    "            plt.ylabel('poission Dif')\n",
    "            plt.show()\n",
    "            plt.clf()\n",
    "\n",
    "        del df1\n",
    "\n",
    "        #torch.save(generator.state_dict(), f='./saved_parameters/gen_test')\n",
    "        # print(pois_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def trainn(generator,\n",
    "          discriminator,\n",
    "          train_data,\n",
    "          val_data,\n",
    "          output_gen_path,\n",
    "          output_disc_path,\n",
    "          output_loss_path,\n",
    "          batch_size=1000,\n",
    "          start_epoch=0,\n",
    "          num_epochs=1000,\n",
    "          num_disc_steps=2,\n",
    "          num_gen_steps=1,\n",
    "          noise_size=128,\n",
    "          l2_regularization=0.001,\n",
    "          learning_rate=0.001,\n",
    "          ):\n",
    "    generator, discriminator = to_cuda_if_available(generator, discriminator)\n",
    "\n",
    "    optim_gen = Adam(generator.parameters(), weight_decay=l2_regularization, lr=learning_rate)\n",
    "    optim_disc = Adam(discriminator.parameters(), weight_decay=l2_regularization, lr=learning_rate)\n",
    "\n",
    "    logger = Logger(output_loss_path, append=start_epoch > 0)\n",
    "\n",
    "    for epoch_index in range(start_epoch, num_epochs):\n",
    "        logger.start_timer()\n",
    "\n",
    "        # train\n",
    "        generator.train(mode=True)\n",
    "        discriminator.train(mode=True)\n",
    "\n",
    "        disc_losses = []\n",
    "        gen_losses = []\n",
    "\n",
    "        more_batches = True\n",
    "        train_data_iterator = train_data.batch_iterator(batch_size)\n",
    "\n",
    "        while more_batches:\n",
    "            # train discriminator\n",
    "            for _ in range(num_disc_steps):\n",
    "                # next batch\n",
    "                try:\n",
    "                    batch = next(train_data_iterator)\n",
    "                except StopIteration:\n",
    "                    more_batches = False\n",
    "                    break\n",
    "\n",
    "                optim_disc.zero_grad()\n",
    "\n",
    "                # first train the discriminator only with real data\n",
    "                real_features = Variable(torch.from_numpy(batch))\n",
    "                real_features = to_cuda_if_available(real_features)\n",
    "                real_pred = discriminator(real_features)\n",
    "                real_loss = - real_pred.mean(0).view(1)\n",
    "                real_loss.backward()\n",
    "\n",
    "                # then train the discriminator only with fake data\n",
    "                noise = Variable(torch.FloatTensor(len(batch), noise_size).normal_())\n",
    "                noise = to_cuda_if_available(noise)\n",
    "                fake_features = generator(noise, training=True)\n",
    "                fake_features = fake_features.detach()  # do not propagate to the generator\n",
    "                fake_pred = discriminator(fake_features)\n",
    "                fake_loss = fake_pred.mean(0).view(1)\n",
    "                fake_loss.backward()\n",
    "\n",
    "                # this is the magic from WGAN-GP\n",
    "                gradient_penalty = calculate_gradient_penalty(discriminator, penalty, real_features, fake_features)\n",
    "                gradient_penalty.backward()\n",
    "\n",
    "                # finally update the discriminator weights\n",
    "                # using two separated batches is another trick to improve GAN training\n",
    "                optim_disc.step()\n",
    "\n",
    "                disc_loss = real_loss + fake_loss + gradient_penalty\n",
    "                disc_loss = to_cpu_if_available(disc_loss)\n",
    "                disc_losses.append(disc_loss.data.numpy())\n",
    "\n",
    "                del disc_loss\n",
    "                del gradient_penalty\n",
    "                del fake_loss\n",
    "                del real_loss\n",
    "\n",
    "            # train generator\n",
    "            for _ in range(num_gen_steps):\n",
    "                optim_gen.zero_grad()\n",
    "\n",
    "                noise = Variable(torch.FloatTensor(len(batch), noise_size).normal_())\n",
    "                noise = to_cuda_if_available(noise)\n",
    "                gen_features = generator(noise, training=True)\n",
    "                fake_pred = discriminator(gen_features)\n",
    "                fake_loss = - fake_pred.mean(0).view(1)\n",
    "                fake_loss.backward()\n",
    "\n",
    "                optim_gen.step()\n",
    "\n",
    "                fake_loss = to_cpu_if_available(fake_loss)\n",
    "                gen_losses.append(fake_loss.data.numpy())\n",
    "\n",
    "                del fake_loss\n",
    "\n",
    "        # log epoch metrics for current class\n",
    "        logger.log(epoch_index, num_epochs, \"discriminator\", \"train_mean_loss\", np.mean(disc_losses))\n",
    "        logger.log(epoch_index, num_epochs, \"generator\", \"train_mean_loss\", np.mean(gen_losses))\n",
    "\n",
    "        # save models for the epoch\n",
    "        with DelayedKeyboardInterrupt():\n",
    "            torch.save(generator.state_dict(), output_gen_path)\n",
    "            torch.save(discriminator.state_dict(), output_disc_path)\n",
    "            logger.flush()\n",
    "\n",
    "    logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainn(generator,\n",
    "          discriminator,\n",
    "          train,\n",
    "          val,\n",
    "          'generator',\n",
    "          'discriminator',\n",
    "          'loss'\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(generated_data.data.to('cpu').numpy().round(2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(generated_data.data.to('cpu').numpy().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(ss.inverse_transform(df1), columns = train.columns)\n",
    "df2 = back_from_dummies(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2['ClaimNb'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2901202.5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform back\n",
    "train = pd.DataFrame(ss.inverse_transform(df1), columns = train.columns)\n",
    "train = back_from_dummies(train)\n",
    "train['ClaimNb'] = train['ClaimNb'].astype(float).astype(int)\n",
    "train['Density'] = train['Density'].clip(lower=1)\n",
    "glm_train = prep.data_cleaning_frequency_schelldorfer(train)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

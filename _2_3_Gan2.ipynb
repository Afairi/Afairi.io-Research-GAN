{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "This script runs the multi GAN and allows you to step through each part\n",
    "# divide y by exposure in xpxixpy\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "# import modules\n",
    "import torch.optim as optim\n",
    "from patsy import dmatrices\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from Functions import dataprep as prep\n",
    "from logging import Logger\n",
    "from torch.optim import Adam\n",
    "from Functions.mcwgan.functions import to_cuda_if_available, to_cpu_if_available, DelayedKeyboardInterrupt, Logger, Dataset\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "import math\n",
    "import joblib\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "## Import created modules\n",
    "from Functions.MC_WGAN_GP.gan_scripts.auto_loader import PolicyDataset\n",
    "from Functions.MC_WGAN_GP.gan_scripts.generator2_v2 import Generator2\n",
    "from Functions.MC_WGAN_GP.gan_scripts.discriminator2_v3 import Discriminator2\n",
    "from Functions.MC_WGAN_GP.gan_scripts.gradiant_penalty import calculate_gradient_penalty\n",
    "from Functions.MC_WGAN_GP.gan_scripts.undo_dummy import back_from_dummies\n",
    "from Functions import metrics\n",
    "\n",
    "from torch.autograd.variable import Variable\n",
    "\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    devid = torch.cuda.current_device()\n",
    "    torch.cuda.set_device(devid)\n",
    "    dev = torch.device(\"cuda\")\n",
    "else:\n",
    "    dev = torch.device(\"cpu\")\n",
    "    \n",
    "print(f'Device: {dev}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle(\"./data/gan_dataprep/train_gan.pickle\")\n",
    "val = pd.read_pickle(\"./data/gan_dataprep/val_gan.pickle\")\n",
    "ss = joblib.load('./data/gan_dataprep/scaler.pickle')\n",
    "pol_dat = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Dataset(train)\n",
    "val = Dataset(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def glm_metrics(train, val):\n",
    "    # Transform back\n",
    "    train = pd.DataFrame(ss.inverse_transform(train), columns = train.columns)\n",
    "    val = pd.DataFrame(ss.inverse_transform(val), columns = val.columns)\n",
    "        \n",
    "    train = back_from_dummies(train)\n",
    "    val = back_from_dummies(val)\n",
    "    \n",
    "    train['ClaimNb'] = train['ClaimNb'].astype(float).astype(int)\n",
    "    val['ClaimNb'] = val['ClaimNb'].astype(float).astype(int)\n",
    "    \n",
    "    train['Density'] = train['Density'].clip(lower=1)\n",
    "\n",
    "    glm_train = prep.data_cleaning_frequency_schelldorfer(train)\n",
    "    glm_val = prep.data_cleaning_frequency_schelldorfer(val)\n",
    "\n",
    "    # Train GLM\n",
    "    formula1 = \"ClaimNb ~ VehPowerGLM + C(VehAgeGLM, Treatment(reference=2)) + C(DrivAgeGLM, Treatment(reference=5)) + BonusMalusGLM + VehBrand + VehGas + DensityGLM + C(Region, Treatment(reference='R24')) + AreaGLM\"\n",
    "    glm1 = smf.glm(formula=formula1, data=glm_train, family=sm.families.Poisson(link=sm.families.links.log()), offset=np.log(glm_train['Exposure'])).fit()\n",
    "    mets = metrics.poisson_deviance(glm1.predict(glm_val, offset=np.log(glm_val['Exposure'])), glm_val['ClaimNb'])\n",
    "    \n",
    "    return mets\n",
    "\n",
    "def xpxixpy(X, y):\n",
    "    return np.dot(np.linalg.inv(np.dot(X.T, X)), np.dot(X.T, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'Dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mglm_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m, in \u001b[0;36mglm_metrics\u001b[1;34m(train, val)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mglm_metrics\u001b[39m(train, val):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Transform back\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m, columns \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m      4\u001b[0m     val \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(ss\u001b[38;5;241m.\u001b[39minverse_transform(val), columns \u001b[38;5;241m=\u001b[39m val\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m      6\u001b[0m     train \u001b[38;5;241m=\u001b[39m back_from_dummies(train)\n",
      "File \u001b[1;32m~\\.conda\\envs\\thesis10\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:1034\u001b[0m, in \u001b[0;36mStandardScaler.inverse_transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1031\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1033\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m-> 1034\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1035\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1036\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1037\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1038\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1039\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1040\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1043\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[1;32m~\\.conda\\envs\\thesis10\\lib\\site-packages\\sklearn\\utils\\validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    877\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 879\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    881\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    882\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    883\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\thesis10\\lib\\site-packages\\sklearn\\utils\\_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    182\u001b[0m     xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(array)\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy.array_api\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'Dataset'"
     ]
    }
   ],
   "source": [
    "glm_metrics(train, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "output_size = [1, 1, 1, 1, 1, 1, 5, 11, 2, 22, 6]\n",
    "# Information about the size of the data\n",
    "data_size = pol_dat.shape[1]  # number of cols in pol_dat\n",
    "var_locs = [0, 1, 2, 3, 4, 5]  # tells us where the continous variables are\n",
    "\n",
    "# parameters\n",
    "z_size = 100  # how big is the random vector fed into the generator\n",
    "# we should only need the 55?\n",
    "batch_size = 100\n",
    "temperature = None  # comes into play with the categorical activation see multioutput.py\n",
    "\n",
    "# Generator tuning\n",
    "gen_hidden_sizes = [100, 100, 100]\n",
    "gen_bn_decay = .90\n",
    "gen_l2_regularization = 0.001\n",
    "gen_learning_rate = 0.01\n",
    "noise_size = z_size\n",
    "\n",
    "assert sum(output_size) == data_size\n",
    "\n",
    "# Discriminator tuning\n",
    "disc_hidden_sizes = [data_size, data_size]\n",
    "disc_bn_decay = .90\n",
    "critic_bool = True  # if false then between 0 and 1\n",
    "mini_batch_bool = False\n",
    "disc_leaky_param = 0.2\n",
    "disc_l2_regularization = 0.001\n",
    "l2_regularization = disc_l2_regularization\n",
    "disc_learning_rate = 0.01\n",
    "penalty = 0.1  ## deals with gradiant penalty\n",
    "epochs = 20000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_data = PolicyDataset(pol_dat, var_locs)\n",
    "\n",
    "# initilize generator and discriminator\n",
    "generator = Generator2(\n",
    "    noise_size=noise_size,\n",
    "    output_size=output_size,\n",
    "    hidden_sizes=gen_hidden_sizes,\n",
    "    bn_decay=gen_bn_decay\n",
    ").to(dev)\n",
    "\n",
    "discriminator = Discriminator2(\n",
    "    input_size=data_size,\n",
    "    hidden_sizes=disc_hidden_sizes,\n",
    "    bn_decay=disc_bn_decay,  # no batch normalization for the critic\n",
    "    critic=critic_bool,  # Do you want a critic\n",
    "    leaky_param=disc_leaky_param,  # parameter for leakyRelu\n",
    "    mini_batch=mini_batch_bool,  # Do you want any mini batch extras\n",
    "    add_rows=56  # Number of rows to add if appending extra rows\n",
    ").to(dev)\n",
    "\n",
    "generator = generator.to(dev)\n",
    "discriminator = discriminator.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                         \r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'optim_disc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m featlen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(real_features)\n\u001b[0;32m     20\u001b[0m shuffled_features \u001b[38;5;241m=\u001b[39m real_features\n\u001b[1;32m---> 21\u001b[0m \u001b[43moptim_disc\u001b[49m\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     22\u001b[0m optim_gen\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Shuffling of features\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'optim_disc' is not defined"
     ]
    }
   ],
   "source": [
    "generator.train(mode=True)\n",
    "discriminator.train(mode=True)\n",
    "disc_losses = []\n",
    "gen_losses = []\n",
    "pois_metric = []\n",
    "# %%\n",
    "disc_loss = torch.tensor(9999)\n",
    "gen_loss = torch.tensor(9999)\n",
    "loop = tqdm(total=epochs, position=0, leave=False)\n",
    "\n",
    "generator = generator.to(dev)\n",
    "discriminator = discriminator.to(dev)\n",
    "\n",
    "cats, conts = auto_data.get_catcont()\n",
    "batch = torch.cat([conts, cats], 1)\n",
    "real_features = Variable(batch)\n",
    "real_features = real_features.to(dev)\n",
    "featlen = len(real_features)\n",
    "\n",
    "shuffled_features = real_features\n",
    "optim_disc.zero_grad()\n",
    "optim_gen.zero_grad()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Shuffling of features\n",
    "    rand_indx = torch.randperm(featlen)\n",
    "    shuffled_features = shuffled_features[rand_indx]\n",
    "\n",
    "    # For each batch\n",
    "    for real_batch in torch.split(shuffled_features, batch_size):\n",
    "        real_batch_size = len(real_batch)\n",
    "\n",
    "        # Discriminator\n",
    "\n",
    "        # train discriminator with real data\n",
    "        real_pred = discriminator(real_batch)\n",
    "        # the disc outputs high numbers if it thinks the data is real, we take the negative of this\n",
    "        # Because we are minimizing loss\n",
    "        real_loss = -real_pred.mean(0).view(1)\n",
    "        real_loss.backward()\n",
    "\n",
    "        # then train the discriminator only with fake data\n",
    "        noise = Variable(torch.FloatTensor(real_batch_size, z_size).normal_()).to(dev)\n",
    "        fake_features = generator(noise, training=True)\n",
    "        fake_features = fake_features.detach().to(dev)  # do not propagate to the generator\n",
    "        fake_pred = discriminator(fake_features)\n",
    "        fake_loss = fake_pred.mean(0).view(1)\n",
    "        fake_loss.backward()\n",
    "\n",
    "        # this is the magic from WGAN-GP\n",
    "        gradient_penalty = calculate_gradient_penalty(discriminator.to(dev), penalty, real_batch.to(dev), fake_features.to(dev))\n",
    "        gradient_penalty.backward()\n",
    "\n",
    "        # finally update the discriminator weights\n",
    "        optim_disc.step()\n",
    "\n",
    "        disc_loss = real_loss + fake_loss + gradient_penalty\n",
    "        disc_losses += [disc_loss]\n",
    "        # Delete to prevent memory leakage\n",
    "        del real_pred\n",
    "        del real_loss\n",
    "        del noise\n",
    "        del fake_features\n",
    "        del fake_pred\n",
    "        del fake_loss\n",
    "        del gradient_penalty\n",
    "        del disc_loss\n",
    "\n",
    "\n",
    "        noise = Variable(torch.FloatTensor(real_batch_size, z_size).normal_()).to(dev)\n",
    "        gen_features = generator(noise)\n",
    "        gen_features = gen_features.to(dev)\n",
    "        gen_pred = discriminator(gen_features)\n",
    "\n",
    "        gen_loss = - gen_pred.mean(0).view(1)\n",
    "        gen_loss.backward()\n",
    "\n",
    "        optim_gen.step()\n",
    "\n",
    "        gen_losses += [gen_loss]\n",
    "        del noise\n",
    "        del gen_features\n",
    "        del gen_pred\n",
    "        del gen_loss\n",
    "\n",
    "    loop.set_description(f'epoch:{epoch}, disc_loss:{disc_losses}, gen_loss:{gen_losses}')\n",
    "    loop.update(1)\n",
    "\n",
    "    # analyze poisson regression parameters every 20 epochs\n",
    "    if False & (epoch % 20 == 0) & (epoch > 10):\n",
    "        with torch.no_grad():\n",
    "            generated_data = generator(Variable(torch.FloatTensor(pol_dat.shape[0], z_size).normal_()).to(dev), training=False)\n",
    "        df1 = pd.DataFrame(generated_data.data.to('cpu').numpy().round(2))\n",
    "        df1.columns = list(pol_dat)\n",
    "        \n",
    "        try:\n",
    "            dev_gen = glm_metrics(df1, val)\n",
    "            print(f'Generated Deviance: {dev_gen}')\n",
    "        except ValueError:\n",
    "            print('No value found yet')\n",
    "        \n",
    "        #print(f'Real poisson prediction: {dev_real}')\n",
    "        #print(f'Generative poisson prediction: {dev_gen}')\n",
    "        #, real_poiss: {dev_real}, gen_poiss: {dev_gen}')\n",
    "\n",
    "        if (epoch > 3):\n",
    "            plt.plot(dev_gen, label='train')\n",
    "            plt.ylabel('poission Dif')\n",
    "            plt.show()\n",
    "            plt.clf()\n",
    "\n",
    "        del df1\n",
    "\n",
    "        #torch.save(generator.state_dict(), f='./saved_parameters/gen_test')\n",
    "        # print(pois_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainn(generator,\n",
    "          discriminator,\n",
    "          train_data,\n",
    "          val_data,\n",
    "          output_gen_path,\n",
    "          output_disc_path,\n",
    "          output_loss_path,\n",
    "          batch_size=1000,\n",
    "          start_epoch=0,\n",
    "          num_epochs=1000,\n",
    "          num_disc_steps=2,\n",
    "          num_gen_steps=1,\n",
    "          noise_size=128,\n",
    "          l2_regularization=0.001,\n",
    "          learning_rate=0.001,\n",
    "          ):\n",
    "    generator, discriminator = to_cuda_if_available(generator, discriminator)\n",
    "\n",
    "    optim_gen = Adam(generator.parameters(), weight_decay=l2_regularization, lr=learning_rate)\n",
    "    optim_disc = Adam(discriminator.parameters(), weight_decay=l2_regularization, lr=learning_rate)\n",
    "\n",
    "    logger = Logger(output_loss_path, append=start_epoch > 0)\n",
    "\n",
    "    for epoch_index in range(start_epoch, num_epochs):\n",
    "        logger.start_timer()\n",
    "\n",
    "        # train\n",
    "        generator.train(mode=True)\n",
    "        discriminator.train(mode=True)\n",
    "\n",
    "        disc_losses = []\n",
    "        gen_losses = []\n",
    "\n",
    "        more_batches = True\n",
    "        train_data_iterator = train_data.batch_iterator(batch_size)\n",
    "\n",
    "        while more_batches:\n",
    "            # train discriminator\n",
    "            for _ in range(num_disc_steps):\n",
    "                # next batch\n",
    "                try:\n",
    "                    batch = next(train_data_iterator)\n",
    "                except StopIteration:\n",
    "                    more_batches = False\n",
    "                    break\n",
    "\n",
    "                optim_disc.zero_grad()\n",
    "\n",
    "                # first train the discriminator only with real data\n",
    "                real_features = Variable(torch.from_numpy(batch))\n",
    "                real_features = to_cuda_if_available(real_features)\n",
    "                real_pred = discriminator(real_features)\n",
    "                real_loss = - real_pred.mean(0).view(1)\n",
    "                real_loss.backward()\n",
    "\n",
    "                # then train the discriminator only with fake data\n",
    "                noise = Variable(torch.FloatTensor(len(batch), noise_size).normal_())\n",
    "                noise = to_cuda_if_available(noise)\n",
    "                fake_features = generator(noise, training=True)\n",
    "                fake_features = fake_features.detach()  # do not propagate to the generator\n",
    "                fake_pred = discriminator(fake_features)\n",
    "                fake_loss = fake_pred.mean(0).view(1)\n",
    "                fake_loss.backward()\n",
    "\n",
    "                # this is the magic from WGAN-GP\n",
    "                gradient_penalty = calculate_gradient_penalty(discriminator, penalty, real_features, fake_features)\n",
    "                gradient_penalty.backward()\n",
    "\n",
    "                # finally update the discriminator weights\n",
    "                # using two separated batches is another trick to improve GAN training\n",
    "                optim_disc.step()\n",
    "\n",
    "                disc_loss = real_loss + fake_loss + gradient_penalty\n",
    "                disc_loss = to_cpu_if_available(disc_loss)\n",
    "                disc_losses.append(disc_loss.data.numpy())\n",
    "\n",
    "                del disc_loss\n",
    "                del gradient_penalty\n",
    "                del fake_loss\n",
    "                del real_loss\n",
    "\n",
    "            # train generator\n",
    "            for _ in range(num_gen_steps):\n",
    "                optim_gen.zero_grad()\n",
    "\n",
    "                noise = Variable(torch.FloatTensor(len(batch), noise_size).normal_())\n",
    "                noise = to_cuda_if_available(noise)\n",
    "                gen_features = generator(noise, training=True)\n",
    "                fake_pred = discriminator(gen_features)\n",
    "                fake_loss = - fake_pred.mean(0).view(1)\n",
    "                fake_loss.backward()\n",
    "\n",
    "                optim_gen.step()\n",
    "\n",
    "                fake_loss = to_cpu_if_available(fake_loss)\n",
    "                gen_losses.append(fake_loss.data.numpy())\n",
    "\n",
    "                del fake_loss\n",
    "\n",
    "        # log epoch metrics for current class\n",
    "        logger.log(epoch_index, num_epochs, \"discriminator\", \"train_mean_loss\", np.mean(disc_losses))\n",
    "        logger.log(epoch_index, num_epochs, \"generator\", \"train_mean_loss\", np.mean(gen_losses))\n",
    "\n",
    "        # save models for the epoch\n",
    "        with DelayedKeyboardInterrupt():\n",
    "            torch.save(generator.state_dict(), output_gen_path)\n",
    "            torch.save(discriminator.state_dict(), output_disc_path)\n",
    "            logger.flush()\n",
    "\n",
    "    logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Functions.mcwgan.functions.Dataset at 0x1f4ffdbc4c0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Int64Index([452907, 166332, 178006, 254527, 431367, 398225, 136781, 344467,\\n            166952,  33347,\\n            ...\\n            314459, 397343, 129981, 430318,  87133, 112167, 325102,  23057,\\n            347486,  19426],\\n           dtype='int64', length=1000)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m          \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m          \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m          \u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgenerator\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdiscriminator\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m      8\u001b[0m \u001b[43m          \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 42\u001b[0m, in \u001b[0;36mtrainn\u001b[1;34m(generator, discriminator, train_data, val_data, output_gen_path, output_disc_path, output_loss_path, batch_size, start_epoch, num_epochs, num_disc_steps, num_gen_steps, noise_size, l2_regularization, learning_rate)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_disc_steps):\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# next batch\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 42\u001b[0m         batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m     44\u001b[0m         more_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\thesis2\\Functions\\mcwgan\\functions.py:168\u001b[0m, in \u001b[0;36mDatasetIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    166\u001b[0m batch_end \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_start\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbatch_end\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\thesis10\\lib\\site-packages\\pandas\\core\\frame.py:3813\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3811\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3812\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 3813\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   3815\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\.conda\\envs\\thesis10\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6070\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6067\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6068\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6070\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6072\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6074\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\thesis10\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6130\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[0;32m   6129\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 6130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6132\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6133\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Int64Index([452907, 166332, 178006, 254527, 431367, 398225, 136781, 344467,\\n            166952,  33347,\\n            ...\\n            314459, 397343, 129981, 430318,  87133, 112167, 325102,  23057,\\n            347486,  19426],\\n           dtype='int64', length=1000)] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "trainn(generator,\n",
    "          discriminator,\n",
    "          train,\n",
    "          val,\n",
    "          'generator',\n",
    "          'discriminator',\n",
    "          'loss'\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(generated_data.data.to('cpu').numpy().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(ss.inverse_transform(df1), columns = train.columns)\n",
    "df2 = back_from_dummies(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2['ClaimNb'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2901202.5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform back\n",
    "train = pd.DataFrame(ss.inverse_transform(df1), columns = train.columns)\n",
    "train = back_from_dummies(train)\n",
    "train['ClaimNb'] = train['ClaimNb'].astype(float).astype(int)\n",
    "train['Density'] = train['Density'].clip(lower=1)\n",
    "glm_train = prep.data_cleaning_frequency_schelldorfer(train)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

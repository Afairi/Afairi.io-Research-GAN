{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec5811cf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "This script runs the multi GAN and allows you to step through each part\n",
    "# divide y by exposure in xpxixpy\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "# import modules\n",
    "import torch.optim as optim\n",
    "from patsy import dmatrices\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "## Import created modules\n",
    "from Functions.MC_WGAN_GP.gan_scripts.auto_loader import PolicyDataset\n",
    "from Functions.MC_WGAN_GP.gan_scripts.generator2_v2 import Generator2\n",
    "from Functions.MC_WGAN_GP.gan_scripts.discriminator2_v3 import Discriminator2\n",
    "from Functions.MC_WGAN_GP.gan_scripts.gradiant_penalty import calculate_gradient_penalty\n",
    "from Functions.MC_WGAN_GP.gan_scripts.undo_dummy import back_from_dummies\n",
    "\n",
    "from torch.autograd.variable import Variable\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71aeb2f8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_pickle(\"./data/gan_dataprep/train_gan.pickle\")\n",
    "val = pd.read_pickle(\"./data/gan_dataprep/val_gan.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "837c3f9c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "487648"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49929541",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pol_dat = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e97583bf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "formula = 'ClaimNb ~ VehBrand + VehGas + Region + AreaGLM + VehPower + VehAge + DrivAge + DensityGLM + BonusMalus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b514a00f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Wrangle train data\n",
    "td = back_from_dummies(train)\n",
    "td['ClaimNb'] = td['ClaimNb'].astype('float').astype('int')\n",
    "y_real, X_real = dmatrices(formula,\n",
    "                 data=td,\n",
    "                 return_type='dataframe')\n",
    "def xpxixpy(X,y):\n",
    "            return np.dot(np.linalg.inv(np.dot(X.T,X)), np.dot(X.T,y))\n",
    "xy = xpxixpy(X_real,y_real)\n",
    "disc_add_rows = xy.shape[0]\n",
    "\n",
    "# Fit a poisson Model\n",
    "poisson_mod = sm.GLM(y_real,X_real,family = sm.families.Poisson(), offset = td['Exposure']).fit()\n",
    "original_params = poisson_mod.params\n",
    "\n",
    "lower = poisson_mod.params - 1.96*poisson_mod.bse  \n",
    "upper = poisson_mod.params + 1.96*poisson_mod.bse \n",
    "\n",
    "# Wrangle Test Data\n",
    "test2 = back_from_dummies(val)\n",
    "test2['ClaimNb'] = test2['ClaimNb'].astype('float').astype('int')\n",
    "y_test, X_test = dmatrices(formula,\n",
    "                 data=test2,\n",
    "                 return_type='dataframe')\n",
    "y_test_resp = np.squeeze(y_test)/np.squeeze(test2['Exposure'])\n",
    "\n",
    "# make predictions on test data with models trained on train data\n",
    "real_pois_preds = poisson_mod.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68942f85",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This next section contains everything that we can tune in the GAN\n",
    "\"\"\"\n",
    "\n",
    "# Information about the size of the data\n",
    "data_size = pol_dat.shape[1] # number of cols in pol_dat \n",
    "var_locs = [0,1,2,3,4,5] # tells us where the continous variables are \n",
    "\n",
    "\n",
    "# parameters\n",
    "z_size = 100 # how big is the random vector fed into the generator\n",
    "# we should only need the 55?\n",
    "batch_size = 1500\n",
    "temperature = None # comes into play with the categorical activation see multioutput.py\n",
    "\n",
    "# Generator tuning\n",
    "gen_hidden_sizes = [100,100,100]\n",
    "gen_bn_decay = .25\n",
    "gen_l2_regularization = 0.1\n",
    "gen_learning_rate = 0.001\n",
    "noise_size = z_size\n",
    "output_size = [1,1,1,1,1,1,5,11,2,22,6]  # how many categories with in each variable\n",
    "\n",
    "assert sum(output_size) == data_size\n",
    "\n",
    "# Discriminator tuning\n",
    "disc_hidden_sizes = [data_size,data_size]\n",
    "disc_bn_decay = .2\n",
    "critic_bool = True # if false then between 0 and 1\n",
    "mini_batch_bool = False\n",
    "disc_leaky_param = 0.2\n",
    "disc_l2_regularization = 0.0\n",
    "disc_learning_rate = 0.001\n",
    "penalty = 1 ## deals with gradiant penalty\n",
    "\n",
    "auto_data = PolicyDataset(pol_dat, var_locs)\n",
    "auto_loader = DataLoader(auto_data,\n",
    "                         batch_size = batch_size,\n",
    "                         pin_memory = True, \n",
    "                         shuffle = True\n",
    "                         )\n",
    "\n",
    "# initilize generator and discriminator\n",
    "generator = Generator2(\n",
    "    noise_size = noise_size,\n",
    "    output_size =  output_size,\n",
    "    hidden_sizes = gen_hidden_sizes,\n",
    "    bn_decay = gen_bn_decay\n",
    "    )\n",
    "\n",
    "discriminator = Discriminator2(\n",
    "    input_size = data_size,\n",
    "    hidden_sizes= disc_hidden_sizes,\n",
    "    bn_decay = disc_bn_decay,               # no batch normalization for the critic\n",
    "    critic = critic_bool,                   # Do you want a critic\n",
    "    leaky_param = disc_leaky_param,         # parameter for leakyRelu\n",
    "    mini_batch = mini_batch_bool,           # Do you want any mini batch extras\n",
    "    add_rows = disc_add_rows # Number of rows to add if appending extra rows \n",
    "    )\n",
    "\n",
    "\n",
    "optim_gen = optim.Adam(generator.parameters(),\n",
    "                       weight_decay= gen_l2_regularization,\n",
    "                       lr= gen_learning_rate\n",
    "                       )\n",
    "\n",
    "optim_disc = optim.Adam(discriminator.parameters(),\n",
    "                        weight_decay= disc_l2_regularization,\n",
    "                        lr= disc_learning_rate\n",
    "                        )    \n",
    "\n",
    "epochs = 500\n",
    "disc_epochs = 2\n",
    "gen_epochs = 1\n",
    "generator.train(mode=True)\n",
    "discriminator.train(mode=True)\n",
    "disc_losses = []\n",
    "gen_losses = []\n",
    "pois_metric = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63432ea",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "disc_loss = torch.tensor(9999)\n",
    "gen_loss = torch.tensor(9999)\n",
    "loop = tqdm(total = epochs, position = 0, leave = False)\n",
    "for epoch in range(epochs):\n",
    "    for d_epoch in range(disc_epochs):  \n",
    "        for c1,c2 in auto_loader: # c1 is continous variables and c2 is the categorical variables\n",
    "            batch = torch.cat([c2,c1],1) \n",
    "            optim_disc.zero_grad()\n",
    "            \n",
    "            # train discriminator with real data\n",
    "            real_features = Variable(batch)\n",
    "            real_pred = discriminator(real_features)\n",
    "            # the disc outputs high numbers if it thinks the data is real, we take the negative of this\n",
    "            # Because we are minimizing loss\n",
    "            real_loss = -real_pred.mean(0).view(1)  \n",
    "            real_loss.backward()\n",
    "\n",
    "            # then train the discriminator only with fake data\n",
    "            noise = Variable(torch.FloatTensor(len(batch), z_size).normal_())\n",
    "            fake_features = generator(noise, training = True)\n",
    "            fake_features = fake_features.detach()  # do not propagate to the generator\n",
    "            fake_pred = discriminator(fake_features)\n",
    "            fake_loss = fake_pred.mean(0).view(1)\n",
    "            fake_loss.backward()\n",
    "            \n",
    "            # this is the magic from WGAN-GP\n",
    "            gradient_penalty = calculate_gradient_penalty(discriminator, penalty, real_features, fake_features)\n",
    "            gradient_penalty.backward()\n",
    "\n",
    "            # finally update the discriminator weights\n",
    "            optim_disc.step()\n",
    "\n",
    "            disc_loss = real_loss + fake_loss + gradient_penalty\n",
    "            disc_losses = disc_loss.item()\n",
    "            # Delete to prevent memory leakage\n",
    "            del gradient_penalty\n",
    "            del fake_loss\n",
    "            del real_loss\n",
    "            del disc_loss\n",
    "            del real_features\n",
    "            del real_pred\n",
    "            del noise\n",
    "            del fake_features\n",
    "            del fake_pred\n",
    "    \n",
    "\n",
    "    for g_epoch in range(gen_epochs):\n",
    "        optim_gen.zero_grad()\n",
    "\n",
    "        noise = Variable(torch.FloatTensor(len(batch), z_size).normal_())\n",
    "        gen_features = generator(noise)\n",
    "        gen_pred = discriminator(gen_features)\n",
    "\n",
    "        gen_loss = - gen_pred.mean(0).view(1)\n",
    "        gen_loss.backward()\n",
    "\n",
    "        optim_gen.step()\n",
    "\n",
    "        gen_loss = gen_loss\n",
    "        gen_losses = gen_loss.item()\n",
    "        del gen_loss \n",
    "        del noise\n",
    "        del gen_features\n",
    "        del gen_pred\n",
    "            \n",
    "    loop.set_description('epoch:{}, disc_loss:{:.4f}, gen_loss:{:.4f}'.format(epoch, disc_losses, gen_losses))\n",
    "    loop.update(1) \n",
    "    # analyze poisson regression parameters every 20 epochs\n",
    "    if(epoch % 20 == 0):\n",
    "        with torch.no_grad():\n",
    "            generated_data = generator(Variable(torch.FloatTensor(pol_dat.shape[0], z_size).normal_()), training = False)\n",
    "        df1 = pd.DataFrame(generated_data.data.numpy())\n",
    "        df1.columns = list(pol_dat)\n",
    "        df2 = back_from_dummies(df1)\n",
    "        df2['ClaimNb'] = df2['ClaimNb'].astype('float').astype('int')\n",
    "        y_gen, X_gen = dmatrices(formula,\n",
    "                             data=df2,\n",
    "                             return_type='dataframe')\n",
    "        \n",
    "        #df2.to_csv(output_data_save_path)\n",
    "        # Fit poisson Model\n",
    "        try:\n",
    "            poisson_mod_gen = sm.GLM(y_gen,X_gen,family = sm.families.Poisson(), offset = np.log(df2['Exposure'])).fit()\n",
    "        except ValueError:\n",
    "            continue\n",
    "        # Calculate Errors\n",
    "        errors_pois = poisson_mod_gen.predict(X_test) - real_pois_preds\n",
    "        \n",
    "        pois_metric.append(round(np.mean(errors_pois), 4))\n",
    "        \n",
    "        if(epoch > 3) :\n",
    "            plt.subplot(311)\n",
    "            plt.plot(pois_metric, label = 'train')\n",
    "            plt.ylabel('poission Dif')\n",
    "            plt.show()\n",
    "            plt.clf()\n",
    "        \n",
    "        print('Mean Absolute Difference Pois:', round(np.mean(errors_pois), 2))\n",
    "        \n",
    "        del errors_pois\n",
    "        del poisson_mod_gen\n",
    "        del importances_gen\n",
    "        del gen_predictions \n",
    "        del generated_data\n",
    "        del df1\n",
    "        del df2\n",
    "        del gen_features\n",
    "        \n",
    "        \n",
    "        torch.save(generator.state_dict(), f='./saved_parameters/gen_test')\n",
    "        print(pois_df)\n",
    "\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b586d181",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
